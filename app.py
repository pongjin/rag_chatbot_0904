import os
import tempfile
import hashlib
import shutil
import streamlit as st
import streamlit.components.v1 as components
import pandas as pd
import json
import numpy as np
import sys

# RAG ê´€ë ¨ imports
from langchain_core.documents import Document
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_openai import OpenAIEmbeddings, ChatOpenAI
from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain.chains.combine_documents import create_stuff_documents_chain
from langchain.chains import create_history_aware_retriever, create_retrieval_chain
from langchain_core.runnables.history import RunnableWithMessageHistory
from langchain_community.chat_message_histories.streamlit import StreamlitChatMessageHistory
from langchain_core.runnables import RunnableMap
from langchain_core.embeddings import Embeddings
from langchain_community.cross_encoders import HuggingFaceCrossEncoder
from langchain_community.vectorstores import Chroma
from langchain_community.embeddings import FakeEmbeddings
from typing import List, Sequence
from sentence_transformers import SentenceTransformer

import hashlib
import shutil

from langchain.retrievers import BM25Retriever, EnsembleRetriever, ContextualCompressionRetriever
from langchain.retrievers.document_compressors import CrossEncoderReranker

from langchain.schema import Document
from langchain_core.runnables import Runnable

from kiwipiepy import Kiwi

# íŒŒì¼ í•´ì‹œ ìƒì„±
def get_file_hash(uploaded_file):
    file_content = uploaded_file.read()
    uploaded_file.seek(0)
    return hashlib.md5(file_content).hexdigest()

# pysqlite3 íŒ¨ì¹˜
__import__('pysqlite3')
sys.modules['sqlite3'] = sys.modules.pop('pysqlite3')

from langchain_chroma import Chroma   # âœ… Chroma import
os.environ["OPENAI_API_KEY"] = st.secrets['OPENAI_API_KEY']


st.set_page_config(page_title="RAG Chatbot", page_icon="ğŸ§ ", layout="wide")

# CSV ë¡œë”© â†’ ìœ ì € ë‹¨ìœ„ë¡œ ë¬¸ì„œ ìƒì„±
@st.cache_resource
def load_csv_and_create_docs(file_path: str, cache_buster: str):
    df = pd.read_csv(file_path)

    if 'user_id' not in df.columns or 'SPLITTED' not in df.columns or 'highlighted_ans' not in df.columns:
        st.error("í•´ë‹¹í•˜ëŠ” ì»¬ëŸ¼ ì—†ìŒ")
        return []

    docs = []
    for idx, row in df.iterrows():
        content = str(row['SPLITTED'])  # í•œ í–‰ì˜ SPLITTED ê°’
        metadata = {
                "source": f"row_{idx}",
                "ans": str(row['highlighted_ans']),
                   }  # í–‰ ì¸ë±ìŠ¤ë¥¼ ì†ŒìŠ¤ë¡œ ì‚¬ìš©
        docs.append(Document(page_content=content, metadata=metadata))
    return docs

@st.cache_resource
def get_embedder():
    class STEmbedding(Embeddings):
        def __init__(self, model_name: str):
            # ì•ˆë‚´ ë©”ì‹œì§€ ì¶œë ¥
            with st.spinner(f"ì„ë² ë”© ëª¨ë¸ì„ ë¡œë“œí•˜ëŠ” ì¤‘ì…ë‹ˆë‹¤... ({model_name})"):
                try:
                    self.model = SentenceTransformer(model_name)
                    st.success(f"ì„ë² ë”© ëª¨ë¸ ë¡œë“œ ì„±ê³µ: {model_name}")
                except Exception as e:
                    st.error(f"ì„ë² ë”© ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {str(e)}")
                    raise e

        def embed_documents(self, texts):
            # ë¦¬ìŠ¤íŠ¸ ì…ë ¥ì— ëŒ€í•´ ë°°ì¹˜ ì¸ì½”ë”©
            return self.model.encode(list(texts), normalize_embeddings=True).tolist()

        def embed_query(self, text):
            # ë‹¨ì¼ ì¿¼ë¦¬ ì¸ì½”ë”©
            return self.model.encode(text, normalize_embeddings=True).tolist()

    return STEmbedding("dragonkue/snowflake-arctic-embed-l-v2.0-ko")

# ë²¡í„°ìŠ¤í† ì–´ ìƒì„±
@st.cache_resource
def create_vector_store(file_path: str, cache_buster: str):
    docs = load_csv_and_create_docs(file_path, cache_buster)
    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=0)
    split_docs = text_splitter.split_documents(docs)

    file_hash = os.path.splitext(os.path.basename(file_path))[0]
    collection_name = f"coll_{file_hash}"

    # ì“°ê¸° ê°€ëŠ¥í•œ ë£¨íŠ¸ (ì˜ˆ: /tmp)
    persist_root = os.path.join(tempfile.gettempdir(), "chroma_db_user")
    persist_dir = os.path.join(persist_root, collection_name)

    # í´ë” ê¹¨ë—í•˜ê²Œ ì¬ìƒì„±
    shutil.rmtree(persist_dir, ignore_errors=True)
    os.makedirs(persist_dir, exist_ok=True)

    embeddings = get_embedder()
    vectorstore = Chroma.from_documents(
        split_docs,
        embeddings,
        collection_name=collection_name,
        persist_directory=None,
    )
    return vectorstore, split_docs  # split_docsë„ í•¨ê»˜ ë°˜í™˜

# BM25 ìš© í•œêµ­ì–´ í† í¬ë‚˜ì´ì €
@st.cache_resource
def get_kiwi():
    return Kiwi()

kiwi = get_kiwi()

# Kiwië¡œ í˜•íƒœì†Œë§Œ ì¶”ì¶œí•˜ëŠ” í•¨ìˆ˜
def tokenize(text):
    # ì²« ë²ˆì§¸ ë¶„ì„ ê²°ê³¼ì—ì„œ í˜•íƒœì†Œë§Œ ì¶”ì¶œ
    return [morph for morph, pos, start, length in result if pos.startswith(("NN", "VV", "VA"))]

# RAG ì²´ì¸ ì´ˆê¸°í™”
@st.cache_resource
def initialize_components(file_path: str, selected_model: str, cache_buster: str):
    vectorstore, split_docs = create_vector_store(file_path, cache_buster)

    # BM25Retriever ìƒì„± (ì›ë¬¸ ìœ ì§€ + tokenizer ì§€ì •)
    bm25_retriever = BM25Retriever.from_documents(
        documents=split_docs,         # Document ê°ì²´ ë¦¬ìŠ¤íŠ¸ë¥¼ ì§ì ‘ ì „ë‹¬
        preprocess_func=tokenize
    )
    bm25_retriever.k = 15  # BM25Retrieverì˜ ê²€ìƒ‰ ê²°ê³¼ ê°œìˆ˜ë¥¼ 20ìœ¼ë¡œ ì„¤ì •

    # Chroma retriever ìƒì„±
    chroma_retriever = vectorstore.as_retriever(search_kwargs={"k": 15})
    
    # ì•™ìƒë¸” retriever ì´ˆê¸°í™”
    ensemble_retriever = EnsembleRetriever(
        retrievers=[bm25_retriever, chroma_retriever],
        weights=[0.2, 0.8],  # BM25: 20%, Chroma: 80%
    )

    # --- ìœ„ì—ì„œ ì •ì˜í•œ ì»¤ìŠ¤í…€ í´ë˜ìŠ¤ ---
    class CrossEncoderRerankerWithScore(CrossEncoderReranker):
        """ì ìˆ˜ë¥¼ ë©”íƒ€ë°ì´í„°ì— ì¶”ê°€í•˜ëŠ” CrossEncoderReranker"""
        def compress_documents(
            self, documents: Sequence[Document], query: str, callbacks=None
        ) -> Sequence[Document]:
            if not documents: return []
            doc_list = [doc.page_content for doc in documents]
            _scores = self.model.score(list(zip([query] * len(doc_list), doc_list)))
            docs_with_scores = sorted(zip(documents, _scores), key=lambda x: x[1], reverse=True)

            result = []
            for doc, score in docs_with_scores[: self.top_n]:
                # ğŸ‘‡ [ìˆ˜ì •] ì ìˆ˜ê°€ 0.0010ì„ ë„˜ëŠ” ë¬¸ì„œë§Œ ê²°ê³¼ì— ì¶”ê°€í•˜ë„ë¡ ìˆ˜ì •
                if score > 0.01:
                    doc.metadata["relevance_score"] = score
                    result.append(doc)
            return result

    @st.cache_resource
    def get_cross_encoder():
        return HuggingFaceCrossEncoder(model_name="dragonkue/bge-reranker-v2-m3-ko")
    
    model = get_cross_encoder()
    compressor = CrossEncoderRerankerWithScore(model=model, top_n=30)
    compression_retriever = ContextualCompressionRetriever(
        base_compressor=compressor, base_retriever=ensemble_retriever
    )
    
    qa_prompt = ChatPromptTemplate.from_messages([
        ("system", "ë‹¤ìŒ ë¬¸ì„œ ë‚´ìš©ì„ ì°¸ê³ í•˜ì—¬ ì§ˆë¬¸ì— ë¬´ì¡°ê±´ í•œêµ­ì–´ë¡œ ë‹µë³€í•´ì¤˜. ë¬¸ì„œì™€ ìœ ì‚¬í•œ ë‚´ìš©ì´ ì—†ìœ¼ë©´ ë¬´ì¡°ê±´ 'ê´€ë ¨ëœ ë‚´ìš©ì´ ì—†ìŠµë‹ˆë‹¤'ë¼ê³  ë§í•´ì¤˜. ê¼­ ì´ëª¨ì§€ ì¨ì¤˜! ì°¸ê³  ë¬¸ì„œëŠ” ì•„ë˜ì— ë³´ì—¬ì¤„ ê±°ì•¼.\n\n{context}"),
        ("human", "{input}"),
    ])
    llm = ChatOpenAI(model=selected_model)

    # retrieverê°€ ë°”ë¡œ ë¬¸ì„œ ë‚´ìš©ì„ {context}ì— ì±„ì›Œì£¼ëŠ” ì—­í• ì„ í•©ë‹ˆë‹¤.
    question_answer_chain = create_stuff_documents_chain(llm, qa_prompt)

    # [ìˆ˜ì •] history_aware_retriever ëŒ€ì‹  ì¤€ë¹„ëœ compression_retrieverë¥¼ ì§ì ‘ ì—°ê²°í•©ë‹ˆë‹¤.
    rag_chain = create_retrieval_chain(compression_retriever, question_answer_chain)

    return rag_chain



def main():

    st.title("ğŸ§  RAG ì§ˆì˜ì‘ë‹µ")
    st.subheader("ì„¤ë¬¸ ì‘ë‹µì„ ì˜ë¯¸ ë‹¨ìœ„ë¡œ ë¶„ë¦¬í•œë’¤(semantic chuncking) í‚¤ì›Œë“œë¥¼ ë„ì¶œí•˜ê³ , ì´ë¥¼ í™œìš©í•˜ì—¬ ë¶„ì„ì„ ì§„í–‰í•©ë‹ˆë‹¤.")
    st.text("ì˜ˆì‹œ) ìœ ì €A: 'ê·¸ë˜í”½ì€ ì¢‹ì§€ë§Œ ì‚¬ìš´ë“œëŠ” ë³„ë¡œì…ë‹ˆë‹¤' -> ìœ ì €AëŠ” 'ê·¸ë˜í”½ì€ ì¢‹ë‹¤' ì™€ 'ì‚¬ìš´ë“œëŠ” ë³„ë¡œë‹¤' ë‘ ê°€ì§€ ì£¼ì œë¥¼ ì–˜ê¸°í•˜ê³  ìˆìŠµë‹ˆë‹¤. LLMì„ í™œìš©í•˜ì—¬ ì´ë¥¼ ì˜ë¯¸ ë‹¨ìœ„ë¡œ ë¶„ë¦¬(ì´í•˜ 'ì²­í¬')í•˜ëŠ” ì „ì²˜ë¦¬ë¥¼ ì§„í–‰í•˜ì˜€ìŠµë‹ˆë‹¤.")
    st.markdown("---")

    # íŒŒì¼ ì—…ë¡œë“œ
    uploaded_file = st.file_uploader(
        "CSV íŒŒì¼ì„ ì—…ë¡œë“œí•˜ì„¸ìš”. ìƒˆë¡­ê²Œ íŒŒì¼ì„ ë„£ëŠ” ê²½ìš°, ì¢Œì¸¡ ìƒë‹¨ ìƒˆë¡œê³ ì¹¨ ë²„íŠ¼ì„ ëˆ„ë¥´ì„¸ìš”", 
        type=['csv'],
        help="user_id, total_cl, name, keywords, summary, SPLITTED ì»¬ëŸ¼ í•„ìš”"
    )

    if uploaded_file is not None:
        try:
            # CSV íŒŒì¼ ì½ê¸°
            df = pd.read_csv(uploaded_file)

            # ì»¬ëŸ¼ í™•ì¸ (name ì»¬ëŸ¼ ì¶”ê°€)
            mindmap_columns = ['user_id', 'total_cl', 'name', 'keywords', 'summary', 'SPLITTED']
            has_mindmap_columns = all(col in df.columns for col in mindmap_columns)

            if not has_mindmap_columns:
                st.error("ë§ˆì¸ë“œë§µ ë˜ëŠ” RAG ê¸°ëŠ¥ì„ ìœ„í•œ í•„ìˆ˜ ì»¬ëŸ¼ì´ ì—†ìŠµë‹ˆë‹¤.")
                st.info("user_id, total_cl, name, keywords, summary, SPLITTED")
                st.stop()

            
            st.subheader("ğŸ“Š ë°ì´í„° ìš”ì•½")
            
            if has_mindmap_columns:
                # ê¸°ë³¸ ì •ë³´ ë©”íŠ¸ë¦­
                filtered_df = df[df.total_cl != 99]
                col1, col2 = st.columns(2)
                with col1:
                    st.metric("ì „ì²´ ì‘ë‹µ ìˆ˜", df.user_id.nunique())
                with col2:
                    st.metric("ì „ì²´ ì²­í¬ ìˆ˜", len(df))

            if has_mindmap_columns:
                # Summary Table (4ë‹¨ê³„ êµ¬ì¡°)
                st.subheader("ğŸ“‹ í‚¤ì›Œë“œ ë¯¸ë¶„ë¥˜ ì²­í¬")
                st.text("í‚¤ì›Œë“œë¡œ ë¶„ë¥˜ë˜ì§€ ì•Šì€ ì²­í¬ë“¤ì„ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.(í…Œì´ë¸” ìš°ì¸¡ ìƒë‹¨ ë‹¤ìš´ë¡œë“œ ê°€ëŠ¥)")
                no_filtered_df = df[["user_id","SPLITTED"]]
                st.dataframe(
                    no_filtered_df.set_index("user_id"),
                    use_container_width=True,
                )

            st.subheader("ğŸ¤– RAG ì§ˆì˜ì‘ë‹µ")
            st.text("ì²­í¬ë¥¼ ê·¼ê±°ë¡œ ìœ ì €ì˜ ì§ˆì˜ì— ì‘ë‹µí•˜ë©°, ì‘ë‹µì— ì‚¬ìš©ëœ ì²­í¬ë¥¼ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.(í˜„ì¬ ìƒìœ„ 10ê°œë§Œ í™•ì¸ ê°€ëŠ¥)")
            st.markdown("RAG êµ¬ì¶• ê°„ ì‹œê°„ì´ ì†Œìš”ë©ë‹ˆë‹¤.(ì•½ Në¶„)")
            
            file_hash = get_file_hash(uploaded_file)

            # ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™”
            if "chat_session_nonce" not in st.session_state:
                st.session_state["chat_session_nonce"] = 0
            
            # íŒŒì¼ì´ ë°”ë€Œë©´ íˆìŠ¤í† ë¦¬ ì´ˆê¸°í™”
            if st.session_state.get("last_file_hash") != file_hash:
                # ê¸°ì¡´ íˆìŠ¤í† ë¦¬ í‚¤ê°€ ìˆìœ¼ë©´ ì œê±°
                old_key = st.session_state.get("chat_history_key")
                if old_key and old_key in st.session_state:
                    del st.session_state[old_key]
                st.session_state["last_file_hash"] = file_hash
                st.session_state["chat_session_nonce"] = 0  # íŒŒì¼ ë°”ë€Œë©´ nonce ì´ˆê¸°í™”
            
            # í˜„ì¬ ì„¸ì…˜ ì‹ë³„ì(íŒŒì¼ í•´ì‹œ + nonce)
            chat_session_id = f"{file_hash}-{st.session_state['chat_session_nonce']}"
            chat_history_key = f"chat_messages_{chat_session_id}"
            
            # ì´ ê°’ì„ ì €ì¥í•´ë‘ë©´ ë‹¤ìŒ í„´ì—ì„œ ì ‘ê·¼ ê°€ëŠ¥
            st.session_state["chat_history_key"] = chat_history_key
            
            temp_dir = tempfile.gettempdir()
            temp_path = os.path.join(temp_dir, f"{file_hash}.csv")

            with open(temp_path, "wb") as f:
                f.write(uploaded_file.getbuffer())

            rag_chain = initialize_components(temp_path, "gpt-4o-mini", cache_buster=file_hash)
            chat_history = StreamlitChatMessageHistory(key=chat_history_key)
            config = {"configurable": {"session_id": chat_session_id}}

            conversational_rag_chain = RunnableWithMessageHistory(
                rag_chain,
                lambda session_id: chat_history,
                input_messages_key="input",
                history_messages_key="history",
                output_messages_key="answer",
            )

            # ì±„íŒ… ì´ˆê¸°í™”/ìƒˆ ì„¸ì…˜ ì‹œì‘ ë²„íŠ¼
            btn_col1, btn_col2 = st.columns([1, 1])
            with btn_col1:
                if st.button("ì±„íŒ… íˆìŠ¤í† ë¦¬ ì§€ìš°ê¸°", use_container_width=True):
                    chat_history.clear()  # í˜„ì¬ ì„¸ì…˜ì˜ ë©”ì‹œì§€ ë¹„ì›€
                    st.rerun()
            
            with btn_col2:
                if st.button("ìƒˆ ì±„íŒ… ì‹œì‘", use_container_width=True):
                    st.session_state["chat_session_nonce"] += 1  # ìƒˆ ì„¸ì…˜
                    # ë©”ëª¨ë¦¬ì— ë‚¨ì•„ìˆëŠ” í˜„ì¬ í‚¤ ì •ë¦¬(ì„ íƒ)
                    if chat_history_key in st.session_state:
                        del st.session_state[chat_history_key]
                    st.rerun()

            if len(chat_history.messages) == 0:
                chat_history.add_ai_message("ì—…ë¡œë“œëœ ìœ ì € ì‘ë‹µ ê¸°ë°˜ìœ¼ë¡œ ë¬´ì—‡ì´ë“  ë¬¼ì–´ë³´ì„¸ìš”! ğŸ¤—")

            for msg in chat_history.messages:
                st.chat_message(msg.type).write(msg.content)

            if prompt_message := st.chat_input("ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”"):
                st.chat_message("human").write(prompt_message)
                with st.chat_message("ai"):
                    with st.spinner("ìƒê° ì¤‘ì…ë‹ˆë‹¤..."):

                        response = conversational_rag_chain.invoke(
                            {"input": prompt_message},
                            config,
                        )
                        answer = response['answer']
                        st.write(answer)

                        if "ê´€ë ¨ëœ ë‚´ìš©ì´ ì—†ìŠµë‹ˆë‹¤" not in answer and response.get("context"):
                            with st.expander("ì°¸ê³  ë¬¸ì„œ í™•ì¸"):
                                seen = set()
                                for doc in response['context']:
                                    key = (doc.metadata.get("source"), doc.page_content)
                                    if key in seen:
                                        continue
                                    seen.add(key)
                                
                                    source = doc.metadata.get('source', 'ì•Œ ìˆ˜ ì—†ìŒ')
                                    raw_ans = doc.metadata.get('ans', 'ì•Œ ìˆ˜ ì—†ìŒ')
                                    #score = doc.metadata.get('score', None)
                                    source_filename = os.path.basename(source)
                                
                                    st.markdown(f"ğŸ‘¤ {source_filename}")
                                    st.html(raw_ans)

        except Exception as e:
            st.error(f"íŒŒì¼ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}")
            st.exception(e)

    else:
        # ìƒ˜í”Œ ì •ë³´ í‘œì‹œ
        col1, col2 = st.columns([1, 1])

        with col1:
            st.info("ğŸ’¡ CSV íŒŒì¼ì„ ì—…ë¡œë“œí•˜ë©´ 4ë‹¨ê³„ í•˜ì´ë¸Œë¦¬ë“œ ë§ˆì¸ë“œë§µê³¼ RAG ì±—ë´‡ì„ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")

            with st.expander("ğŸŒ³ 4ë‹¨ê³„ í•˜ì´ë¸Œë¦¬ë“œ ë§ˆì¸ë“œë§µì˜ íŠ¹ì§•"):
                st.markdown("""
                **ğŸ—ï¸ 4ë‹¨ê³„ íŠ¸ë¦¬ + Force êµ¬ì¡°**
                - ë©”ì¸ ì£¼ì œê°€ ì™¼ìª½ì— ìœ„ì¹˜
                - ìƒìœ„ê°œë…(name)ì´ ì²« ë²ˆì§¸ í™•ì¥
                - í‚¤ì›Œë“œë“¤ì´ ë‘ ë²ˆì§¸ í™•ì¥ 
                - ìš”ì•½ë“¤ì´ ì„¸ ë²ˆì§¸ í™•ì¥
                - Force Simulationìœ¼ë¡œ ê²¹ì¹¨ ë°©ì§€
                
                **ğŸ¯ ì¸í„°ë™ì…˜**  
                - ë©”ì¸ ì£¼ì œ í´ë¦­ â†’ ëª¨ë“  ìƒìœ„ê°œë… í‘œì‹œ
                - ìƒìœ„ê°œë… í´ë¦­ â†’ í•´ë‹¹ í‚¤ì›Œë“œë“¤ í‘œì‹œ
                - í‚¤ì›Œë“œ í´ë¦­ â†’ í•´ë‹¹ ìš”ì•½ë“¤ í‘œì‹œ
                - ë“œë˜ê·¸ë¡œ ë…¸ë“œ ììœ  ì´ë™
                - íŠ¸ë¦¬ ë³µì›ìœ¼ë¡œ ì–¸ì œë“  ì›ë˜ í˜•íƒœ ë³µê·€
                - ë¬¼ë¦¬ì—”ì§„ í† ê¸€ë¡œ ê²¹ì¹¨ ë°©ì§€ ì œì–´
                """)

        with col2:
            with st.expander("ğŸ“‹ CSV íŒŒì¼ í˜•ì‹ ìš”êµ¬ì‚¬í•­ (4ë‹¨ê³„)"):
                st.markdown("""
                **ë§ˆì¸ë“œë§µìš© (í•„ìˆ˜):**
                ```
                user_id, total_cl, name, keywords, summary
                user001, 1, "ì œí’ˆí’ˆì§ˆ", "í’ˆì§ˆ", "ì œí’ˆì´ ë§Œì¡±ìŠ¤ëŸ½ë‹¤"
                user002, 2, "ê°€ê²©ì •ì±…", "ê°€ê²©", "ê°€ê²©ì´ í•©ë¦¬ì ì´ë‹¤"
                user003, 99, "", "", "ë¬´íš¨ ì‘ë‹µ"
                ```
                
                **RAG ì±—ë´‡ìš© (ì¶”ê°€ í•„ìš”):**
                ```
                SPLITTED, highlighted_ans
                "ì œí’ˆì— ëŒ€í•œ ìƒì„¸í•œ ì˜ê²¬...", "ì›ë³¸ ì‘ë‹µ..."
                "ì„œë¹„ìŠ¤ ê²½í—˜ì— ëŒ€í•œ ì„¤ëª…...", "ì›ë³¸ ì‘ë‹µ..."
                ```
                
                **4ë‹¨ê³„ êµ¬ì¡°**: ë©”ì¸ â†’ ìƒìœ„ê°œë…(name) â†’ í‚¤ì›Œë“œ â†’ ìš”ì•½
                * total_cl != 99 ì¸ ë°ì´í„°ë§Œ ë§ˆì¸ë“œë§µì— ì‚¬ìš©ë©ë‹ˆë‹¤
                * ëª¨ë“  ê¸°ëŠ¥ì„ ì‚¬ìš©í•˜ë ¤ë©´ ëª¨ë“  ì»¬ëŸ¼ì´ í•„ìš”í•©ë‹ˆë‹¤
                """)

if st.button("ğŸ”„ ìƒˆë¡œê³ ì¹¨ ë²„íŠ¼ì„ ëˆ„ë¥´ì„¸ìš”"):
    st.cache_resource.clear()
    shutil.rmtree(os.path.join(tempfile.gettempdir(), "chroma_db_user"), ignore_errors=True)
    st.success("ì´ˆê¸°í™” ì™„ë£Œ")
    st.rerun()

if __name__ == "__main__":
    main()
