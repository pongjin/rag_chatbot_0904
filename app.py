import os
import tempfile
import hashlib
import shutil
import streamlit as st
import streamlit.components.v1 as components
import pandas as pd
import json
import numpy as np
import sys
import requests
from wordcloud import WordCloud
import matplotlib.pyplot as plt

# RAG ê´€ë ¨ imports
from langchain_core.documents import Document
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_openai import OpenAIEmbeddings, ChatOpenAI
from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain.chains.combine_documents import create_stuff_documents_chain
from langchain.chains import create_history_aware_retriever, create_retrieval_chain
from langchain_core.runnables.history import RunnableWithMessageHistory
from langchain_community.chat_message_histories.streamlit import StreamlitChatMessageHistory
from langchain_core.runnables import RunnableMap
from langchain_core.embeddings import Embeddings
from langchain_community.cross_encoders import HuggingFaceCrossEncoder
from langchain_community.vectorstores import Chroma
from langchain_community.embeddings import FakeEmbeddings
from typing import List, Sequence
from sentence_transformers import SentenceTransformer

import hashlib
import shutil

from langchain_community.retrievers import BM25Retriever
from langchain.retrievers import EnsembleRetriever, ContextualCompressionRetriever
from langchain.retrievers.document_compressors import CrossEncoderReranker

from langchain.schema import Document
from langchain_core.runnables import Runnable

from kiwipiepy import Kiwi

# íŒŒì¼ í•´ì‹œ ìƒì„±
def get_file_hash(uploaded_file):
    file_content = uploaded_file.read()
    uploaded_file.seek(0)
    return hashlib.md5(file_content).hexdigest()

# pysqlite3 íŒ¨ì¹˜
__import__('pysqlite3')
sys.modules['sqlite3'] = sys.modules.pop('pysqlite3')

from langchain_chroma import Chroma   # âœ… Chroma import
os.environ["OPENAI_API_KEY"] = st.secrets['OPENAI_API_KEY']


st.set_page_config(page_title="RAG Chatbot", page_icon="ğŸ§ ", layout="wide")

# CSV ë¡œë”© â†’ ìœ ì € ë‹¨ìœ„ë¡œ ë¬¸ì„œ ìƒì„±
@st.cache_resource
def load_csv_and_create_docs(file_path: str, cache_buster: str):
    df = pd.read_csv(file_path)

    if 'user_id' not in df.columns or 'SPLITTED' not in df.columns or 'highlighted_ans' not in df.columns:
        st.error("í•´ë‹¹í•˜ëŠ” ì»¬ëŸ¼ ì—†ìŒ")
        return []

    docs = []
    for idx, row in df.iterrows():
        content = str(row['SPLITTED'])  # í•œ í–‰ì˜ SPLITTED ê°’
        metadata = {
                "source": f"row_{idx}",
                "ans": str(row['highlighted_ans']),
                   }  # í–‰ ì¸ë±ìŠ¤ë¥¼ ì†ŒìŠ¤ë¡œ ì‚¬ìš©
        docs.append(Document(page_content=content, metadata=metadata))
    return docs

@st.cache_resource
def get_embedder():
    class STEmbedding(Embeddings):
        def __init__(self, model_name: str):
            # ì•ˆë‚´ ë©”ì‹œì§€ ì¶œë ¥
            with st.spinner(f"ì„ë² ë”© ëª¨ë¸ì„ ë¡œë“œí•˜ëŠ” ì¤‘ì…ë‹ˆë‹¤... ({model_name})"):
                try:
                    self.model = SentenceTransformer(model_name)
                    st.success(f"ì„ë² ë”© ëª¨ë¸ ë¡œë“œ ì„±ê³µ: {model_name}")
                except Exception as e:
                    st.error(f"ì„ë² ë”© ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {str(e)}")
                    raise e

        def embed_documents(self, texts):
            # ë¦¬ìŠ¤íŠ¸ ì…ë ¥ì— ëŒ€í•´ ë°°ì¹˜ ì¸ì½”ë”©
            return self.model.encode(list(texts), normalize_embeddings=True).tolist()

        def embed_query(self, text):
            # ë‹¨ì¼ ì¿¼ë¦¬ ì¸ì½”ë”©
            return self.model.encode(text, normalize_embeddings=True).tolist()

    return STEmbedding("dragonkue/snowflake-arctic-embed-l-v2.0-ko") #sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2

# ë²¡í„°ìŠ¤í† ì–´ ìƒì„±
@st.cache_resource
def create_vector_store(file_path: str, cache_buster: str):
    docs = load_csv_and_create_docs(file_path, cache_buster)
    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=0)
    split_docs = text_splitter.split_documents(docs)

    file_hash = os.path.splitext(os.path.basename(file_path))[0]
    collection_name = f"coll_{file_hash}_{cache_buster}"
    #collection_name = f"coll_{file_hash}"

    # ì“°ê¸° ê°€ëŠ¥í•œ ë£¨íŠ¸ (ì˜ˆ: /tmp)
    persist_root = os.path.join(tempfile.gettempdir(), "chroma_db_user")
    persist_dir = os.path.join(persist_root, collection_name)

    # í´ë” ê¹¨ë—í•˜ê²Œ ì¬ìƒì„±
    shutil.rmtree(persist_dir, ignore_errors=True)
    os.makedirs(persist_dir, exist_ok=True)

    embeddings = get_embedder()
    vectorstore = Chroma.from_documents(
        split_docs,
        embeddings,
        collection_name=collection_name,
        persist_directory=persist_dir, #None,
    )
    return vectorstore, split_docs  # split_docsë„ í•¨ê»˜ ë°˜í™˜


# BM25 ìš© í•œêµ­ì–´ í† í¬ë‚˜ì´ì €
'''
@st.cache_resource
def get_kiwi():
    return Kiwi()

kiwi = get_kiwi()

# Kiwië¡œ í˜•íƒœì†Œë§Œ ì¶”ì¶œí•˜ëŠ” í•¨ìˆ˜
def tokenize(text):
    # ì²« ë²ˆì§¸ ë¶„ì„ ê²°ê³¼ì—ì„œ í˜•íƒœì†Œë§Œ ì¶”ì¶œ
    result = kiwi.analyze(text)[0][0]
    return [morph for morph, pos, start, length in result if pos.startswith(("NN", "VV", "VA"))]
'''

# RAG ì²´ì¸ ì´ˆê¸°í™”
@st.cache_resource
def initialize_components(file_path: str, selected_model: str, cache_buster: str):
    vectorstore, split_docs = create_vector_store(file_path, cache_buster)

    # BM25Retriever ìƒì„± (ì›ë¬¸ ìœ ì§€ + tokenizer ì§€ì •)
    bm25_retriever = BM25Retriever.from_documents(
        documents=split_docs,         # Document ê°ì²´ ë¦¬ìŠ¤íŠ¸ë¥¼ ì§ì ‘ ì „ë‹¬
        #preprocess_func=tokenize
    )
    bm25_retriever.k = 15  # BM25Retrieverì˜ ê²€ìƒ‰ ê²°ê³¼ ê°œìˆ˜ë¥¼ 20ìœ¼ë¡œ ì„¤ì •

    # Chroma retriever ìƒì„±
    chroma_retriever = vectorstore.as_retriever(search_kwargs={"k": 15})
    
    # ì•™ìƒë¸” retriever ì´ˆê¸°í™”
    ensemble_retriever = EnsembleRetriever(
        retrievers=[bm25_retriever, chroma_retriever],
        weights=[0.2, 0.8],  # BM25: 20%, Chroma: 80%
    )

    # --- ìœ„ì—ì„œ ì •ì˜í•œ ì»¤ìŠ¤í…€ í´ë˜ìŠ¤ ---
    class CrossEncoderRerankerWithScore(CrossEncoderReranker):
        """ì ìˆ˜ë¥¼ ë©”íƒ€ë°ì´í„°ì— ì¶”ê°€í•˜ëŠ” CrossEncoderReranker"""
        def compress_documents(
            self, documents: Sequence[Document], query: str, callbacks=None
        ) -> Sequence[Document]:
            if not documents: return []
            doc_list = [doc.page_content for doc in documents]
            _scores = self.model.score(list(zip([query] * len(doc_list), doc_list)))
            docs_with_scores = sorted(zip(documents, _scores), key=lambda x: x[1], reverse=True)

            result = []
            for doc, score in docs_with_scores[: self.top_n]:
                # ğŸ‘‡ [ìˆ˜ì •] ì ìˆ˜ê°€ 0.0010ì„ ë„˜ëŠ” ë¬¸ì„œë§Œ ê²°ê³¼ì— ì¶”ê°€í•˜ë„ë¡ ìˆ˜ì •
                if score > 0.0:
                    doc.metadata["relevance_score"] = score
                    result.append(doc)
            return result
    '''
    @st.cache_resource
    def get_cross_encoder():
        return HuggingFaceCrossEncoder(model_name="Dongjin-kr/ko-reranker") #  dragonkue/bge-reranker-v2-m3-ko
    
    model = get_cross_encoder()
    compressor = CrossEncoderRerankerWithScore(model=model, top_n=30)
    compression_retriever = ContextualCompressionRetriever(
        base_compressor=compressor, base_retriever=ensemble_retriever
    )
    '''
    qa_prompt = ChatPromptTemplate.from_messages([
        ("system", "ë‹¤ìŒ ë¬¸ì„œ ë‚´ìš©ì„ ì°¸ê³ í•˜ì—¬ ì§ˆë¬¸ì— ë¬´ì¡°ê±´ í•œêµ­ì–´ë¡œ ë‹µë³€í•´ì¤˜. ë¬¸ì„œì™€ ìœ ì‚¬í•œ ë‚´ìš©ì´ ì—†ìœ¼ë©´ ë¬´ì¡°ê±´ 'ê´€ë ¨ëœ ë‚´ìš©ì´ ì—†ìŠµë‹ˆë‹¤'ë¼ê³  ë§í•´ì¤˜. ê¼­ ì´ëª¨ì§€ ì¨ì¤˜! ì°¸ê³  ë¬¸ì„œëŠ” ì•„ë˜ì— ë³´ì—¬ì¤„ ê±°ì•¼.\n\n{context}"),
        ("human", "{input}"),
    ])
    llm = ChatOpenAI(model=selected_model)

    # retrieverê°€ ë°”ë¡œ ë¬¸ì„œ ë‚´ìš©ì„ {context}ì— ì±„ì›Œì£¼ëŠ” ì—­í• ì„ í•©ë‹ˆë‹¤.
    question_answer_chain = create_stuff_documents_chain(llm, qa_prompt)

    # [ìˆ˜ì •] history_aware_retriever ëŒ€ì‹  ì¤€ë¹„ëœ compression_retrieverë¥¼ ì§ì ‘ ì—°ê²°í•©ë‹ˆë‹¤.
    #rag_chain = create_retrieval_chain(compression_retriever , question_answer_chain)
    rag_chain = create_retrieval_chain(ensemble_retriever , question_answer_chain)

    return rag_chain



def main():

    st.title("ğŸ§  ì£¼ê´€ì‹ ë°ì´í„° ê²€ìƒ‰ê¸°")
    st.subheader("ì„¤ë¬¸ ì‘ë‹µì„ ì˜ë¯¸ ë‹¨ìœ„ë¡œ ë¶„ë¦¬í•œë’¤(semantic chuncking) í‚¤ì›Œë“œë¥¼ ë„ì¶œí•˜ê³ , ì´ë¥¼ í™œìš©í•˜ì—¬ ë¶„ì„ì„ ì§„í–‰í•©ë‹ˆë‹¤.")
    st.text("ì˜ˆì‹œ) ìœ ì €A: 'ê·¸ë˜í”½ì€ ì¢‹ì§€ë§Œ ì‚¬ìš´ë“œëŠ” ë³„ë¡œì…ë‹ˆë‹¤' -> ìœ ì €AëŠ” 'ê·¸ë˜í”½ì€ ì¢‹ë‹¤' ì™€ 'ì‚¬ìš´ë“œëŠ” ë³„ë¡œë‹¤' ë‘ ê°€ì§€ ì£¼ì œë¥¼ ì–˜ê¸°í•˜ê³  ìˆìŠµë‹ˆë‹¤. LLMì„ í™œìš©í•˜ì—¬ ì´ë¥¼ ì˜ë¯¸ ë‹¨ìœ„ë¡œ ë¶„ë¦¬(ì´í•˜ 'ì²­í¬')í•˜ëŠ” ì „ì²˜ë¦¬ë¥¼ ì§„í–‰í•˜ì˜€ìŠµë‹ˆë‹¤.")
    st.markdown("---")

    # íŒŒì¼ ì—…ë¡œë“œ
    if "uploader_key" not in st.session_state:
        st.session_state["uploader_key"] = 0
    
    uploaded_file = st.file_uploader(
        "CSV íŒŒì¼ì„ ì—…ë¡œë“œí•˜ì„¸ìš”. ìƒˆë¡­ê²Œ íŒŒì¼ì„ ë„£ëŠ” ê²½ìš°, ì¢Œì¸¡ ìƒë‹¨ ìƒˆë¡œê³ ì¹¨ ë²„íŠ¼ì„ ëˆ„ë¥´ì„¸ìš”", 
        type=['csv'],
        help="user_id, SPLITTED, highlighted_ans ì»¬ëŸ¼ í•„ìš”",
        key=f"file_uploader_{st.session_state['uploader_key']}"  # âœ… ì„¸ì…˜ í‚¤ ì ìš©
    )

    if uploaded_file is not None:
        try:
            # CSV íŒŒì¼ ì½ê¸°
            df = pd.read_csv(uploaded_file)

            # ì»¬ëŸ¼ í™•ì¸ (name ì»¬ëŸ¼ ì¶”ê°€)
            mindmap_columns = ['user_id','highlighted_ans', 'SPLITTED']
            has_mindmap_columns = all(col in df.columns for col in mindmap_columns)

            if not has_mindmap_columns:
                st.error("ë§ˆì¸ë“œë§µ ë˜ëŠ” RAG ê¸°ëŠ¥ì„ ìœ„í•œ í•„ìˆ˜ ì»¬ëŸ¼ì´ ì—†ìŠµë‹ˆë‹¤.")
                st.info("user_id, SPLITTED, highlighted_ans")
                st.stop()

            
            st.subheader("ğŸ“Š ë°ì´í„° ìš”ì•½")
            
            if has_mindmap_columns:
                # ê¸°ë³¸ ì •ë³´ ë©”íŠ¸ë¦­
                col1, col2, col3 = st.columns(3)
                with col1:
                    st.metric("ì „ì²´ ì‘ë‹µ ìˆ˜(ë¶ˆì„±ì‹¤ ì œì™¸)", df[df.keyword != 'ì—†ìŒ'].user_id.nunique())
                with col2:
                    st.metric("ì „ì²´ ì²­í¬ ìˆ˜(ë¶ˆì„±ì‹¤ ì œì™¸)", len(df[df.keyword != 'ì—†ìŒ']))
                with col3:
                    # ìƒìœ„ 10ê°œ í‚¤ì›Œë“œ ì¶”ì¶œ
                    df['clean_keyword'] = df['keyword'].apply(lambda x: x.replace(' ',''))
                    df_cnt = pd.DataFrame(df.groupby('clean_keyword').user_id.nunique().sort_values(ascending= False)).reset_index()
                    top10 = df_cnt[df_cnt.clean_keyword != 'ì—†ìŒ'].head(10)
                    # Noto Sans KR (TTF ë²„ì „) ë‹¤ìš´ë¡œë“œ
                    url = "https://github.com/moonspam/NanumSquare/raw/master/NanumSquareR.ttf"
                    font_path = "NanumSquare.ttf"
                    
                    if not os.path.exists(font_path):
                        r = requests.get(url)
                        with open(font_path, "wb") as f:
                            f.write(r.content)
                    
                    # íŒŒì¼ í¬ê¸° í™•ì¸ (ì •ìƒì ìœ¼ë¡œ ë°›ì•˜ëŠ”ì§€ ì²´í¬)
                    st.text("ì£¼ë¡œ ì–¸ê¸‰ëœ í‚¤ì›Œë“œ")
                    
                    wc = WordCloud(
                        font_path=font_path, 
                        background_color="white", 
                        width=200, 
                        height=100
                    ).generate_from_frequencies(dict(zip(top10['clean_keyword'], top10['user_id'])))
                    
                    # ì‹œê°í™”
                    fig, ax = plt.subplots(figsize=(2, 1))
                    ax.imshow(wc, interpolation='bilinear')
                    ax.axis('off')
                    st.pyplot(fig, use_container_width=False)

            if has_mindmap_columns:

                st.subheader("ğŸ“‹ ì „ì²´ ì²­í¬")
                st.text("ë¶ˆì„±ì‹¤ ì‘ë‹µì„ ì œì™¸í•œ ì „ì²´ ì²­í¬ë“¤ì„ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.(í…Œì´ë¸” ìš°ì¸¡ ìƒë‹¨ ë‚´ ê²€ìƒ‰ ë° ë‹¤ìš´ë¡œë“œ ê°€ëŠ¥)")
                no_filtered_df = df[df.SPLITTED != 'ì—†ìŒ'][["user_id","SPLITTED"]]
                st.dataframe(
                    no_filtered_df.set_index("user_id"),
                    use_container_width=True,
                )

            
            st.subheader("ğŸ¤– RAG ì§ˆì˜ì‘ë‹µ")
            st.text("ì²­í¬ë¥¼ ê·¼ê±°ë¡œ ìœ ì €ì˜ ì§ˆì˜ì— ì‘ë‹µí•˜ë©°, ì‘ë‹µì— ì‚¬ìš©ëœ ì²­í¬ë¥¼ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.(ìµœëŒ€ 30ê°œ ê¹Œì§€ í™•ì¸ ê°€ëŠ¥)")
            st.markdown("ì²­í¬ í¬ê¸°ì— ë”°ë¼ RAG êµ¬ì¶• ì‹œê°„ì´ ì†Œìš”ë©ë‹ˆë‹¤.(ì•½ Në¶„)")
            
            file_hash = get_file_hash(uploaded_file)

            # ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™”
            if "chat_session_nonce" not in st.session_state:
                st.session_state["chat_session_nonce"] = 0
            
            # íŒŒì¼ì´ ë°”ë€Œë©´ íˆìŠ¤í† ë¦¬ ì´ˆê¸°í™”
            if st.session_state.get("last_file_hash") != file_hash:
                # ê¸°ì¡´ íˆìŠ¤í† ë¦¬ í‚¤ê°€ ìˆìœ¼ë©´ ì œê±°
                old_key = st.session_state.get("chat_history_key")
                if old_key and old_key in st.session_state:
                    del st.session_state[old_key]
                st.session_state["last_file_hash"] = file_hash
                st.session_state["chat_session_nonce"] = 0  # íŒŒì¼ ë°”ë€Œë©´ nonce ì´ˆê¸°í™”
            
            # í˜„ì¬ ì„¸ì…˜ ì‹ë³„ì(íŒŒì¼ í•´ì‹œ + nonce)
            chat_session_id = f"{file_hash}-{st.session_state['chat_session_nonce']}"
            chat_history_key = f"chat_messages_{chat_session_id}"
            
            # ì´ ê°’ì„ ì €ì¥í•´ë‘ë©´ ë‹¤ìŒ í„´ì—ì„œ ì ‘ê·¼ ê°€ëŠ¥
            st.session_state["chat_history_key"] = chat_history_key
            
            temp_dir = tempfile.gettempdir()
            temp_path = os.path.join(temp_dir, f"{file_hash}.csv")

            with open(temp_path, "wb") as f:
                f.write(uploaded_file.getbuffer())

            rag_chain = initialize_components(temp_path, "gpt-4o-mini", cache_buster=file_hash)
            chat_history = StreamlitChatMessageHistory(key=chat_history_key)
            config = {"configurable": {"session_id": chat_session_id}}

            conversational_rag_chain = RunnableWithMessageHistory(
                rag_chain,
                lambda session_id: chat_history,
                input_messages_key="input",
                history_messages_key="history",
                output_messages_key="answer",
            )

            if st.button("ì±„íŒ… íˆìŠ¤í† ë¦¬ ì§€ìš°ê¸°", use_container_width=True):
                chat_history.clear()  # í˜„ì¬ ì„¸ì…˜ì˜ ë©”ì‹œì§€ ë¹„ì›€
                st.rerun()

            if len(chat_history.messages) == 0:
                chat_history.add_ai_message("ì—…ë¡œë“œëœ ìœ ì € ì‘ë‹µ ê¸°ë°˜ìœ¼ë¡œ ë¬´ì—‡ì´ë“  ë¬¼ì–´ë³´ì„¸ìš”! ğŸ¤—")

            # íˆìŠ¤í† ë¦¬ ì¶œë ¥
            for msg in chat_history.messages:
                if msg.type == "human":
                    st.chat_message("human").write(msg.content)
                elif msg.type == "ai":
                    try:
                        content = json.loads(msg.content)
                        st.chat_message("ai").write(content["answer"])
            
                        if content.get("context"):
                            with st.expander("ì°¸ê³  ë¬¸ì„œ í™•ì¸", expanded=False):
                                seen = set()
                                for doc in content["context"]:
                                    key = (doc["source"], doc["page_content"])
                                    if key in seen:
                                        continue
                                    seen.add(key)
                                    st.markdown(f"ğŸ‘¤ {doc['source']}")
                                    st.html(doc["ans"])
                    except json.JSONDecodeError:
                        st.chat_message("ai").write(msg.content)
            
            if prompt_message := st.chat_input("ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”"):
                #st.chat_message("human").write(prompt_message)
                #with st.chat_message("ai"):
                with st.spinner(f"{prompt_message} ì‘ë‹µ ìƒì„±ì¤‘..."):

                    # ì‚¬ìš©ì ë©”ì‹œì§€ ë¨¼ì € ì¶”ê°€
                    chat_history.add_user_message(prompt_message)
                    
                    # ê¸°ë³¸ rag_chain ì‚¬ìš© (ìë™ íˆìŠ¤í† ë¦¬ ì €ì¥ ì—†ìŒ)
                    response = rag_chain.invoke({"input": prompt_message, "history": chat_history.messages})
                    
                    answer = response['answer']

                    '''
                    response = conversational_rag_chain.invoke(
                        {"input": prompt_message},
                        config,
                    )
                    answer = response['answer']
                    '''
        
                    # Document ê°ì²´ë¥¼ ì§ë ¬í™” ê°€ëŠ¥í•œ dictë¡œ ë³€í™˜
                    context = []
                    if "ê´€ë ¨ëœ ë‚´ìš©ì´ ì—†ìŠµë‹ˆë‹¤" not in answer and response.get("context"):
                        for doc in response["context"]:
                            context.append({
                                "source": doc.metadata.get("source", "ì•Œ ìˆ˜ ì—†ìŒ"),
                                "ans": doc.metadata.get("ans", "ì•Œ ìˆ˜ ì—†ìŒ"),
                                "page_content": doc.page_content
                            })
        
                    # JSONìœ¼ë¡œ ì§ë ¬í™”í•´ì„œ ì €ì¥
                    chat_history.add_ai_message(
                        json.dumps({"answer": answer, "context": context}, ensure_ascii=False)
                    )
                    
                    st.rerun()

        except Exception as e:
            st.error(f"íŒŒì¼ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}")
            st.exception(e)

    else:
        # ìƒ˜í”Œ ì •ë³´ í‘œì‹œ
        col1, col2 = st.columns([1, 1])

if st.button("ğŸ”„ ìƒˆë¡œê³ ì¹¨ (ëª¨ë“  ê¸°ë¡ ì‚­ì œ)"):
    # 1. Streamlitì˜ ë¦¬ì†ŒìŠ¤ ìºì‹œ ì´ˆê¸°í™”
    st.cache_resource.clear()

    # 2. ë””ìŠ¤í¬ì— ì €ì¥ëœ ChromaDB íŒŒì¼ ì‚­ì œ
    # ê²½ë¡œê°€ ì¡´ì¬í•˜ëŠ”ì§€ í™•ì¸ í›„ ì‚­ì œí•˜ëŠ” ê²ƒì´ ë” ì•ˆì „í•©ë‹ˆë‹¤.
    chroma_db_path = os.path.join(tempfile.gettempdir(), "chroma_db_user")
    if os.path.exists(chroma_db_path):
        shutil.rmtree(chroma_db_path, ignore_errors=True)

    # 3. ì„¸ì…˜ ìƒíƒœ(ì±„íŒ… ê¸°ë¡ ë“±) ì™„ì „ ì´ˆê¸°í™”
    # st.session_stateì˜ ëª¨ë“  í‚¤ë¥¼ ìˆœíšŒí•˜ë©° ì‚­ì œí•©ë‹ˆë‹¤.
    for key in list(st.session_state.keys()):
        del st.session_state[key]

    # 4. íŒŒì¼ ì—…ë¡œë” í‚¤ ê°±ì‹  â†’ ì—…ë¡œë“œ í‘œì‹œ ì§€ì›€
    st.session_state["uploader_key"] = st.session_state.get("uploader_key", 0) + 1  
    
    st.success("âœ… ëª¨ë“  ìºì‹œì™€ ì±„íŒ… ê¸°ë¡ì´ ì´ˆê¸°í™”ë˜ì—ˆìŠµë‹ˆë‹¤!")
    st.rerun()

if __name__ == "__main__":
    main()
